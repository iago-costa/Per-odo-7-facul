{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["import the necessary packages"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import argparse\n", "import imutils\n", "import cv2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["construct the argument parse and parse the arguments"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ap = argparse.ArgumentParser()\n", "ap.add_argument(\"-m\", \"--model\", required=True,\n", "\thelp=\"path to deep learning segmentation model\")\n", "ap.add_argument(\"-c\", \"--classes\", required=True,\n", "\thelp=\"path to .txt file containing class labels\")\n", "ap.add_argument(\"-v\", \"--video\", required=True,\n", "\thelp=\"path to input video file\")\n", "ap.add_argument(\"-o\", \"--output\", required=True,\n", "\thelp=\"path to output video file\")\n", "ap.add_argument(\"-l\", \"--colors\", type=str,\n", "\thelp=\"path to .txt file containing colors for labels\")\n", "ap.add_argument(\"-w\", \"--width\", type=int, default=500,\n", "\thelp=\"desired width (in pixels) of input image\")\n", "args = vars(ap.parse_args())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["load the class label names"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["CLASSES = open(args[\"classes\"]).read().strip().split(\"\\n\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["if a colors file was supplied, load it from disk"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if args[\"colors\"]:\n", "\tCOLORS = open(args[\"colors\"]).read().strip().split(\"\\n\")\n", "\tCOLORS = [np.array(c.split(\",\")).astype(\"int\") for c in COLORS]\n", "\tCOLORS = np.array(COLORS, dtype=\"uint8\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["otherwise, we need to randomly generate RGB colors for each class<br>\n", "label"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["else:\n", "\t# initialize a list of colors to represent each class label in\n", "\t# the mask (starting with 'black' for the background/unlabeled\n", "\t# regions)\n", "\tnp.random.seed(42)\n", "\tCOLORS = np.random.randint(0, 255, size=(len(CLASSES) - 1, 3),\n", "\t\tdtype=\"uint8\")\n", "\tCOLORS = np.vstack([[0, 0, 0], COLORS]).astype(\"uint8\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["load our serialized model from disk"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["net = cv2.dnn.readNet(args[\"model\"])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["initialize the video stream and pointer to output video file"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["vs = cv2.VideoCapture(args[\"video\"])\n", "writer = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["loop over frames from the video file stream"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["while True:\n", "\t# read the next frame from the file\n", "\t(grabbed, frame) = vs.read()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t# if the frame was not grabbed, then we have reached the end\n", "\t# of the stream\n", "\tif not grabbed:\n", "\t\tbreak"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t# construct a blob from the frame and perform a forward pass\n", "\t# using the segmentation model\n", "\tframe = imutils.resize(frame, width=args[\"width\"])\n", "\tblob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (1024, 512), 0,\n", "\t\tswapRB=True, crop=False)\n", "\tnet.setInput(blob)\n", "\toutput = net.forward()\n", "\t"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t# infer the total number of classes along with the spatial\n", "\t# dimensions of the mask image via the shape of the output array\n", "\t(numClasses, height, width) = output.shape[1:4]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t# our output class ID map will be num_classes x height x width in\n", "\t# size, so we take the argmax to find the class label with the\n", "\t# largest probability for each and every (x, y)-coordinate in the\n", "\t# image\n", "\tclassMap = np.argmax(output[0], axis=0)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t# given the class ID map, we can map each of the class IDs to its\n", "\t# corresponding color\n", "\tmask = COLORS[classMap]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t# resize the mask such that its dimensions match the original size\n", "\t# of the input frame\n", "\tmask = cv2.resize(mask, (frame.shape[1], frame.shape[0]),\n", "\t\tinterpolation=cv2.INTER_NEAREST)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t# perform a weighted combination of the input frame with the mask\n", "\t# to form an output visualization\n", "\toutput = ((0.3 * frame) + (0.7 * mask)).astype(\"uint8\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t# check if the video writer is None\n", "\tif writer is None:\n", "\t\t# initialize our video writer\n", "\t\tfourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n", "\t\twriter = cv2.VideoWriter(args[\"output\"], fourcc, 30,\n", "\t\t\t(output.shape[1], output.shape[0]), True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["\t# write the output frame to disk\n", "\twriter.write(output)\n", "        \n", "\t#display the output frame\n", "\tcv2.imshow(\"Frame\", output)\n", "\tkey = cv2.waitKey(1) & 0xFF\n", " \n", "\t# if the esc key was pressed, break from the loop\n", "\tif key == 27:\n", "\t       break"]}, {"cell_type": "markdown", "metadata": {}, "source": ["release the file pointers"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["writer.release()\n", "vs.release()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["USAGE<br>\n", "python segment_video.py --model enet-cityscapes/enet-model.net --classes enet-cityscapes/enet-classes.txt --colors enet-cityscapes/enet-colors.txt --video videos/massachusetts.mp4 --output output/massachusetts_output.avi<br>\n", "python segment_video.py --model enet-cityscapes/enet-model.net --classes enet-cityscapes/enet-classes.txt --colors enet-cityscapes/enet-colors.txt --video videos/toronto.mp4 --output output/toronto_output.avi"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}